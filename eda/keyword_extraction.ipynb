{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import wikipediaapi as wk\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "from prompts import load_template\n",
    "from utils import create_experiment_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "WIKI_USER_AGENT = os.getenv(\"WIKI_USER_AGENT\")\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "RESULT_DIR = \"experiments\"\n",
    "TARGET_DATA = \"train_v2.0.1.csv\"\n",
    "EXP_VERSION = \"v1.0.2\"  # 컨벤션에 따라 새롭게 실험할 경우 버저닝 정보를 기록해주세요.\n",
    "\n",
    "PARAGRAPH = \"paragraph\"\n",
    "QUESTION = \"question\"\n",
    "CHOICES = \"choices\"\n",
    "QUESTION_PLUS = \"question_plus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = pd.read_csv(os.path.join(DATA_DIR, TARGET_DATA))\n",
    "datasets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "    template=load_template(file_name=\"wikipedia_search_keyword.txt\", template_type=\"keyword_extraction\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\", temperature=0, max_tokens=None, timeout=None, max_retries=2, api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "keyword_extractor = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keyword(data: pd.Series) -> list[str]:\n",
    "    \"\"\"\n",
    "    문제 해결에 필요한 키워드 5개를 추출하고 결과를 배열로 반환하는 함수\n",
    "\n",
    "    Args:\n",
    "        data (pd.Series): 제공된 데이터셋의 한 행(row), 문제 하나\n",
    "\n",
    "    Returns:\n",
    "        list[str]: 추출된 5개 키워드\n",
    "    \"\"\"\n",
    "    response = keyword_extractor.invoke(\n",
    "        {\n",
    "            \"paragraph\": data[PARAGRAPH],\n",
    "            \"question\": data[QUESTION],\n",
    "            \"question_plus\": data[QUESTION_PLUS],\n",
    "            \"choices\": data[CHOICES],\n",
    "        },\n",
    "    )\n",
    "    response_split = response.content.split(\",\")\n",
    "    keywords = [keyword.strip() for keyword in response_split]\n",
    "    return keywords\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "datasets[\"keywords\"] = datasets.progress_apply(extract_keyword, axis=1)\n",
    "datasets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file_name = create_experiment_file_name(target_file=TARGET_DATA, version=EXP_VERSION)\n",
    "result_file_path = os.path.join(DATA_DIR, RESULT_DIR, result_file_name)\n",
    "datasets.to_csv(result_file_path, index=False)  # 1차 중간 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추출된 키워드가 위키피디아에 존재하는지 여부 확인\n",
    "\n",
    "- 키워드 추출 결과가 유효한지 확인하기 위한 작업입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = wk.Wikipedia(user_agent=WIKI_USER_AGENT, language=\"ko\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 키워드를 유용하게 다루기 위해 키워드 별로 칼럼으로 분리\n",
    "keywords_expanded = pd.DataFrame(datasets[\"keywords\"].tolist(), index=datasets.index)\n",
    "keywords_expanded.columns = [f\"keyword_{i+1}\" for i in range(keywords_expanded.shape[1])]\n",
    "new_datasets = pd.concat([datasets.drop(columns=[\"keywords\"]), keywords_expanded], axis=1)\n",
    "\n",
    "new_datasets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위키피디아 페이지가 있는지 키워드 별로 확인\n",
    "for col in keywords_expanded.columns:\n",
    "    new_datasets[f\"{col}_exists\"] = new_datasets[col].apply(lambda x: wiki.page(x).exists())\n",
    "\n",
    "new_datasets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _exists 열만 추출\n",
    "exists_columns = [col for col in new_datasets.columns if col.endswith(\"_exists\")]\n",
    "\n",
    "true_count = new_datasets[exists_columns].sum().sum()  # True 값의 총합\n",
    "false_count = (new_datasets[exists_columns] == False).sum().sum()  # False 값의 총합\n",
    "\n",
    "true_count, false_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_datasets.to_csv(result_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
