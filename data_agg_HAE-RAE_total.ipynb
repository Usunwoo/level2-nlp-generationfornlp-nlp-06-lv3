{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HAERAE 데이터를 모두 넣어줄 빈 데이터셋을 선언한다\n",
    "train_agg = pd.read_csv(\"./data/agg_other_benchmarks/train_agg.csv\")\n",
    "data_haerae_agg = pd.DataFrame(columns=train_agg.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규표현식으로 특정 패턴의 텍스트를 추출하는 함수를 선언해준다\n",
    "\n",
    "\n",
    "def extract_before(text, keyword):\n",
    "    pattern = f\"(.*){re.escape(keyword)}\"  # 키워드 이전 모든 문자열 매칭\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()  # 키워드 이전 문자열 반환\n",
    "    return None  # 키워드가 없으면 None 반환\n",
    "\n",
    "\n",
    "def extract_after(text, keyword):\n",
    "    pattern = f\"{re.escape(keyword)}(.*)\"  # 키워드 이후 모든 문자열 매칭\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()  # 키워드 이후 문자열 반환\n",
    "    return None  # 키워드가 없으면 None 반환\n",
    "\n",
    "\n",
    "def extract_between(text, start_keyword, end_keyword):\n",
    "    # 시작과 끝 키워드 사이의 텍스트 추출\n",
    "    pattern = f\"{re.escape(start_keyword)}(.*?){re.escape(end_keyword)}\"\n",
    "    match = re.search(pattern, text, re.DOTALL)  # re.DOTALL은 줄바꿈 포함 매칭\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return None\n",
    "\n",
    "\n",
    "# 알파벳을 숫자로 맵핑하는 함수를 만들어준다 (answer 칼럼용)\n",
    "def convert_string_to_number(s):\n",
    "    # 문자열-숫자 매핑 딕셔너리\n",
    "    mapping = {\"(A)\": 1, \"(B)\": 2, \"(C)\": 3, \"(D)\": 4, \"(E)\": 5}\n",
    "    # 매핑에 없는 문자열은 None 반환\n",
    "    return mapping.get(s, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HAERAE 데이터셋 선별 데이터 취합 (agg stands for aggregation)\n",
    "\n",
    "agg_categories = [\n",
    "    \"reading_comprehension\",\n",
    "    \"standard_nomenclature\",\n",
    "    \"correct_definition_matching\",\n",
    "    \"general_knowledge\",\n",
    "    \"history\",\n",
    "    \"rare_words\",\n",
    "]  # 취합할 데이터셋에 맞춰 수정할 것\n",
    "\n",
    "datasets_paths = []\n",
    "for i in agg_categories:\n",
    "    default_path = \"./data/agg_other_benchmarks/to_be_agg/\"\n",
    "    dataset_file_name = \"HAE-RAE_\" + i + \".csv\"\n",
    "    dataset_path = default_path + dataset_file_name\n",
    "    datasets_paths.append(dataset_path)\n",
    "\n",
    "ids = []\n",
    "for i in agg_categories:\n",
    "    default_name = i.replace(\"_\", \"-\")\n",
    "    id = \"haerae-\" + default_name + \"-\"\n",
    "    ids.append(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading_comprehension: 100%|██████████| 936/936 [00:01<00:00, 527.76it/s]\n",
      "standard_nomenclature: 100%|██████████| 153/153 [00:00<00:00, 529.53it/s]\n",
      "correct_definition_matching: 100%|██████████| 439/439 [00:00<00:00, 511.50it/s]\n",
      "general_knowledge: 100%|██████████| 176/176 [00:00<00:00, 523.57it/s]\n",
      "history: 100%|██████████| 188/188 [00:00<00:00, 512.98it/s]\n",
      "rare_words: 100%|██████████| 405/405 [00:00<00:00, 507.41it/s]\n"
     ]
    }
   ],
   "source": [
    "for path_index in range(len(datasets_paths)):\n",
    "    dataset = pd.read_csv(datasets_paths[path_index])\n",
    "    dataset_name = agg_categories[path_index]\n",
    "    count = 0\n",
    "    for i in tqdm(range(len(dataset)), desc=f\"{dataset_name}\"):\n",
    "        id = f\"{ids[path_index]}{count}\"\n",
    "        query = dataset.iloc[i][\"query\"]\n",
    "\n",
    "        if dataset_name == \"reading_comprehension\":\n",
    "            passage = extract_between(query, \"### 지문:\", \"### 질문:\")\n",
    "            question = extract_between(query, \"### 질문:\", \"### 선택지:\")\n",
    "\n",
    "        elif dataset_name == \"correct_definition_matching\":\n",
    "            passage = extract_between(query, \"### 문장:\", \"### 선택지:\")\n",
    "            question = extract_before(query, \"### 문장:\")\n",
    "            question = question.replace(\"다음\", \"위\")\n",
    "\n",
    "        elif dataset_name == \"general_knowledge\":\n",
    "            passage = extract_between(query, \"### 질문:\", \"### 참고:\")\n",
    "            question = extract_before(query, \"### 질문:\")\n",
    "            question = question.replace(\"다음\", \"위\")\n",
    "            question_plus = extract_between(query, \"### 참고:\", \"### 선택지:\")\n",
    "\n",
    "        else:\n",
    "            passage = extract_between(query, \"### 질문:\", \"### 선택지:\")\n",
    "            question = extract_before(query, \"### 질문:\")\n",
    "            question = question.replace(\"다음\", \"위\")\n",
    "\n",
    "        choices = str(dataset.iloc[i][\"options\"])\n",
    "        answer = convert_string_to_number(dataset.iloc[i][\"answer\"])\n",
    "\n",
    "        if dataset_name == \"general_knowledge\":\n",
    "            df_agg = pd.DataFrame(\n",
    "                [\n",
    "                    {\n",
    "                        \"id\": id,\n",
    "                        \"paragraph\": passage,\n",
    "                        \"question\": question,\n",
    "                        \"choices\": choices,\n",
    "                        \"answer\": answer,\n",
    "                        \"question_plus\": question_plus,\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            df_agg = pd.DataFrame(\n",
    "                [\n",
    "                    {\n",
    "                        \"id\": id,\n",
    "                        \"paragraph\": passage,\n",
    "                        \"question\": question,\n",
    "                        \"choices\": choices,\n",
    "                        \"answer\": answer,\n",
    "                        \"question_plus\": None,\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        data_haerae_agg = pd.concat([data_haerae_agg, df_agg])\n",
    "        print(len(data_haerae_agg))\n",
    "        count += 1\n",
    "\n",
    "\n",
    "final_dataset = pd.concat([train_agg, data_haerae_agg])\n",
    "\n",
    "\n",
    "def get_category_initials(name):\n",
    "    name_split = name.split(\"_\")\n",
    "    initials = \"\"\n",
    "    for i in name_split:\n",
    "        initials += i[0]\n",
    "    return initials\n",
    "\n",
    "\n",
    "final_file_name = \"train_agg_haerae\"\n",
    "\n",
    "for i in agg_categories:\n",
    "    final_file_name += \"_\" + get_category_initials(i)\n",
    "\n",
    "final_file_name += \".csv\"\n",
    "final_dataset.to_csv(final_file_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
